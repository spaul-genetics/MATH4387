<!DOCTYPE html>
<html>
<head>
  <title>Diagnostics</title>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Diagnostics',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                      },

      // Author information
      presenters: [
            {
        name:  'Subrata Paul' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <script src="site_libs/header-attrs-2.4/header-attrs.js"></script>
  <link href="site_libs/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="site_libs/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="site_libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/hammer.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="site_libs/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    summary {
      display: list-item;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }
/* https://github.com/ropensci/plotly/pull/524#issuecomment-468142578 */
slide:not(.current) .plotly.html-widget{
  display: block;
}

    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ff0000; font-weight: bold; } /* Alert */
            code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #7d9029; } /* Attribute */
            code span.bn { color: #40a070; } /* BaseN */
            code span.bu { } /* BuiltIn */
            code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4070a0; } /* Char */
            code span.cn { color: #880000; } /* Constant */
            code span.co { color: #60a0b0; font-style: italic; } /* Comment */
            code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #ba2121; font-style: italic; } /* Documentation */
            code span.dt { color: #902000; } /* DataType */
            code span.dv { color: #40a070; } /* DecVal */
            code span.er { color: #ff0000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #40a070; } /* Float */
            code span.fu { color: #06287e; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #007020; font-weight: bold; } /* Keyword */
            code span.op { color: #666666; } /* Operator */
            code span.ot { color: #007020; } /* Other */
            code span.pp { color: #bc7a00; } /* Preprocessor */
            code span.sc { color: #4070a0; } /* SpecialChar */
            code span.ss { color: #bb6688; } /* SpecialString */
            code span.st { color: #4070a0; } /* String */
            code span.va { color: #19177c; } /* Variable */
            code span.vs { color: #4070a0; } /* VerbatimString */
            code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
        
  </style>


</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      
      <p data-config-presenter><!-- populated from slide_config.json --></p>
            <p style="margin-top: 6px; margin-left: -2px;">6/4/2020</p>
          </hgroup>
  </slide>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>

<script>
    $(document).ready(function() {
    $('slide:not(.title-slide, .backdrop, .segue)').append('<footer></footer>');    
    $('footer').attr('url', "https://math5387.web.app");

  })
</script>

<style>
pre {
    display: block;
    font-family: monospace;
    white-space: pre;
    margin: 1em 0px;
    margin-top: 0em;
    margin-right: 0px;
    margin-bottom: -0.2em;
    margin-left: 0px;
}
footer:after {
    font-size: 12pt;
    content: attr(url);
    position: absolute;
    bottom: 5px;
    right: 60px;
    line-height: 1.9;
    display: block;
  }
slides > slide {
  font-family: 'Open Sans', Arial, sans-serif;
  font-size: 26px;
  color: black;
  width: 900px;
  height: 700px;
  margin-left: -450px;
  margin-top: -350px;
  padding: 0px 60px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  -ms-border-radius: 5px;
  -o-border-radius: 5px;
  border-radius: 5px;
  -webkit-transition: all 0.6s ease-in-out;
  -moz-transition: all 0.6s ease-in-out;
  -o-transition: all 0.6s ease-in-out;
  transition: all 0.6s ease-in-out;
}
slides > slide > hgroup + article {
  margin-top: 5px;
}
</style>

<slide class=""><hgroup><h2>What we need to check?</h2></hgroup><article  id="what-we-need-to-check">

<ul>
<li>Nonlinearity of regression function</li>
<li>Nonconstancy of error variance</li>
<li>Nonindependence of error terms</li>
<li>Nonnromality of error terms</li>
<li>Presence of outliers and influencial observations</li>
</ul>

</article></slide><slide class="segue dark nobackground level1"><hgroup class = 'auto-fadein'><h2>Residual Plots</h2></hgroup><article  id="residual-plots">

</article></slide><slide class=""><hgroup><h2>Importance</h2></hgroup><article  id="importance">

<ul>
<li>Diagnostic plots of the response variable are not very useful</li>
<li>Residual plots usually (indirectly) carries out diagnostics of the response variable</li>
<li>Residuals gives us information about model structure and the nature of unobserved randomness.</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Different plots of residuals</h2></hgroup><article  id="different-plots-of-residuals">

<ul>
<li>Plots of residuals against predictor variables</li>
<li>Plot of residuals against fitted values</li>
<li>Plot of absolute or squared residuals against fitted values or predictor variables</li>
<li>Plot of residuals against time or other sequence</li>
<li>Plots of residuals against omited predictor variables</li>
<li>Box plot and density plot of residuals</li>
<li>Q-Q plot of residuals</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Reminder</h2></hgroup><article  id="reminder">

<p><img src="images/simple_lin_reg.png" width="814" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Simulated scenarios</h2></hgroup><article  id="simulated-scenarios">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-2-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Example (Non-constant error variance)</h2></hgroup><article  id="example-non-constant-error-variance">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-3-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Test for non-constatnt variance</h2></hgroup><article  id="test-for-non-constatnt-variance">

<ul>
<li>If the data is separable in some way then a F test works</li>
<li>Brown_Forsythe test (a t-test)</li>
<li>Breusch-Pegan test (also known as Cook-Weisberg score test)

<ul>
<li>It is a chi-squared test</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Residual Plots</h2></hgroup><article  id="residual-plots-1">

<ul>
<li>It can be used to check

<ul>
<li>Nonlinear structure</li>
<li>Non-constant error variance</li>
<li>Non-zero error mean</li>
</ul></li>
<li>Though it is not used directly to detect unusual observations but gives us an overview of the observations.</li>
<li>Though it indicates potential structural problem it does not clearly suggest transformation because it fails to distinguish between monotonic and non-monotonic transformation.</li>
<li>Does not show the nature of the marginal effect of a predictor variable, given the other predictor variables in the model.</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Interesting Extreme Example</h2></hgroup><article  id="interesting-extreme-example">

<pre class = 'prettyprint lang-r'>library(faraway)
set.seed(123)
male = 600
female = 400

bp = c(rnorm(male, 85, 2), rnorm(female, 75, 1))
bmi = c(rnorm(male, 24, 4), rnorm(female, 28, 2))
chol = c(rnorm(male, 200, 10), rnorm(female, 150, 15))
gender = c(rep(1, male), rep(0, female))
dat = data.frame(bp = bp, bmi= bmi ,chol = chol, gender = gender)
lmod = lm(bmi ~ bp + chol, data = dat)
sumary(lmod)</pre>

<pre >##               Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept) 48.4499279  2.1428433 22.6101 &lt; 2.2e-16
## bp          -0.2169564  0.0388197 -5.5888  2.95e-08
## chol        -0.0286526  0.0074002 -3.8719  0.000115
## 
## n = 1000, p = 3, Residual SE = 3.38723, R-Squared = 0.23</pre>

</article></slide><slide class=""><hgroup><h2>Interesting Extreme Example (Residual Plot)</h2></hgroup><article  id="interesting-extreme-example-residual-plot">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-5-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>What does the residual plot suggests?</h2></hgroup><article  id="what-does-the-residual-plot-suggests">

<pre >##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 9.7006468  7.8100284  1.2421  0.21470
## bp          0.1604810  0.0823442  1.9489  0.05177
## chol        0.0038192  0.0168418  0.2268  0.82068
## 
## n = 600, p = 3, Residual SE = 3.89664, R-Squared = 0.01</pre>

<pre >##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 12.0170106  7.8054019  1.5396  0.12446
## bp           0.2005841  0.1027723  1.9517  0.05167
## chol         0.0071092  0.0068681  1.0351  0.30125
## 
## n = 400, p = 3, Residual SE = 2.11171, R-Squared = 0.01</pre>

</article></slide><slide class=""><hgroup><h2>Test for non-constatnt variance</h2></hgroup><article  id="test-for-non-constatnt-variance-1">

<ul>
<li>If the data is separable in some way then a F test works</li>
<li>Brown_Forsythe test (a t-test)</li>
<li>Breusch-Pegan test (also known as Cook-Weisberg score test)

<ul>
<li>It is a chi-squared test</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Added variable plot</h2></hgroup><article  id="added-variable-plot">

<ul>
<li>Gives marginal relationship given the other predictors are in the model</li>
<li>Can be used to investigate if a new variable is worthy to include in the model</li>
<li>Provide clearer picture compared to residual vs predictor plot</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Simulated Scenarios</h2></hgroup><article  id="simulated-scenarios-1">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-7-1.png" width="720" style="display: block; margin: auto;" /></p>

<ol>
<li>No additional information from the predictor</li>
<li>Helpful addition of the predictor</li>
<li>Inclusion of the predictor with some transformation.</li>
</ol>

</article></slide><slide class=""><hgroup><h2>AV plot example</h2></hgroup><article  id="av-plot-example">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-8-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Component plus residual (CR) plot</h2></hgroup><article  id="component-plus-residual-cr-plot">

<p>The <strong>component plus residual (cr) plot</strong> (a.k.a, <strong>partial residual plot</strong>) is a competitor to the added variable plot.</p>

<p>The <strong>cr plot</strong> shows \(\hat{\beta}_i x_i+\hat{\epsilon}\) versus \(x_i\).</p>

<ul>
<li><p><strong>cr</strong> plots are useful for checking nonlinear relationships in the variable being considered for inclusion in the model.</p></li>
<li><p>They can also suggest potential transformation of the data so that the relationship is linear.</p></li>
<li><p>If the scatter plot does not appear to be linear, then there is a nonlinear relationship between the regressor and the response (after accounting for the other regressors).<br/></p></li>
<li><p>The slope of the line fit to the cr plot is \(\hat{\beta}_i\).</p></li>
</ul>

</article></slide><slide class="segue dark nobackground level1"><hgroup class = 'auto-fadein'><h2>Unusual observations</h2></hgroup><article  id="unusual-observations">

</article></slide><slide class=""><hgroup><h2>Leverage</h2></hgroup><article  id="leverage">

<ul>
<li>A <strong>leverage</strong> point is an observation that is unusual in the predictor space.</li>
<li>\(h_{ii}\) is called the leverage value of the \(i\) th observation.</li>
<li>\(h_{ii}\) is a measure of distance between the \(X\) values for the \(i\)th observation from the mean of the \(X\) values of all \(n\) observations.</li>
<li>\(h_{ii}\) measures the role of the \(X\) values in determining how important \(y_i\) is in affecting \(\hat{y}_i\)</li>
<li>A half-normal plot of the leverage values can be used to identify observations with unusually high leverage.</li>
<li>A leverage value \(h_i\) is usually considered large if it is more than twice as large as the mean leverage value.</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-9-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Outlier</h2></hgroup><article  id="outlier">

<p>An outlier is a point that does not fit the current model.</p>

<ul>
<li>An outlier is context specific! An outlier for one model may not be an outlier for a different model.</li>
</ul>

<p>externally <strong>studentized</strong> residual</p>

<p>\[t_i = \frac{y_i -\hat{y}_{(i)}}{\hat{\sigma}_{(i)}\sqrt{1+x_i^T\left(X_{(i)}^TX_{(i)} \right)^{-1}x_i}} \sim T_{n-p-1}.\]</p>

<ul>
<li>Bonferroni correction is needed</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example-1">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-10-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Influential observations</h2></hgroup><article  id="influential-observations">

<ul>
<li><p>An influential observation is one whose removal from the dataset would cause a large change in the fitted model.</p></li>
<li><p>An influential observation is usually a leverage point, an outlier, or both.</p></li>
</ul>

<p>The Cook’s distance is a popular inferential tool because it reduces influence information to a single value for each observation.</p>

<p>The Cook’s distance for the ith observation is</p>

<p>\[D_i = \frac{(\hat{y} - \hat{y}_{(i)})^T(\hat{y} - \hat{y}_{(i)})}{p\hat{\sigma}^2} = \frac{1}{p}r_i^2\frac{h_i}{1-h_i}\]</p>

</article></slide><slide class=""><hgroup><h2>Example</h2></hgroup><article  id="example-2">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-11-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=""><hgroup><h2>Influence plot</h2></hgroup><article  id="influence-plot">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-12-1.png" width="720" style="display: block; margin: auto;" /></p>

<pre >##                  StudRes        Hat      CookD
## Chile         -2.3134295 0.03729796 0.03781324
## Japan          1.6032158 0.22330989 0.14281625
## United States -0.3546151 0.33368800 0.01284481
## Zambia         2.8535583 0.06433163 0.09663275
## Libya         -1.0893033 0.53145676 0.26807042</pre>

</article></slide><slide class=""><hgroup><h2>Correct or Delete the Observation(s)</h2></hgroup><article  id="correct-or-delete-the-observations">

<ul>
<li><p>If they’re data entry errors, correct the problem. If they can’t be fixed, remove them (they’re wrong, so they don’t tell us anything useful).</p></li>
<li><p>Remove them if they’re not part of the population of interest (you are studying dogs, but this observation is a cat).</p></li>
<li><p>Remove them because they break the model.</p></li>
<li><p>This is a bad idea.</p></li>
<li><p>Make sure to indicate that you removed them from the data set and explain why.</p></li>
<li><p><em>THIS IS A BAD IDEA.</em></p></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Fit a Different Model</h2></hgroup><article  id="fit-a-different-model">

<ul>
<li>An outlier/influential point for one model may not be for another.</li>
<li>Examine the physical context—why did it happen?</li>
<li>An outlier/influential point may be interesting in itself.

<ul>
<li>An outlier in a statistical analysis of credit card transactions may indicate fraud!</li>
</ul></li>
<li>This may suggest a better model.</li>
<li>Use robust regression, which is not as affected by outliers/influential observations.</li>
<li>Never automatically remove outliers/influential points!</li>
<li>They provide important information that may otherwise be missed.</li>
<li>Fit the model with and without the influential observation(s).</li>
<li>Do your results substantively change?</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Checking Error</h2></hgroup><article  id="checking-error">

<p>Summary of methods for checking error assumptions</p>

<ul>
<li>Mean-zero error assumption:

<ul>
<li>Plot of residuals versus fitted values</li>
</ul></li>
<li>Constant error variance assumption:

<ul>
<li>Plot of residuals versus fitted values</li>
<li>Plot of √(|ϵ ̂|) versus fitted values.</li>
</ul></li>
<li>Normal error assumption:

<ul>
<li>q-q of residuals</li>
<li>Shapiro-wilk test</li>
</ul></li>
<li>Autocorrelated errors:

<ul>
<li>Plot of residuals versus time</li>
<li>Plot of successive pairs of residuals</li>
<li>Durbin-Watson test</li>
</ul></li>
</ul>

</article></slide><slide class="segue dark nobackground level1"><hgroup class = 'auto-fadein'><h2>Weighted Least Squares</h2></hgroup><article  id="weighted-least-squares">

</article></slide><slide class=""><hgroup><h2>Model</h2></hgroup><article  id="model">

<p>The generalized multiple regression model:</p>

<p>\[Y_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_{p-1}X_{i, p-1} + \epsilon_i\] where,</p>

<ul>
<li>\(\beta_0, \beta_1, \dots , \beta_{p-1}\) are parameters</li>
<li>\(X_{i1}, \dots X_{i, p-1}\) are known constants</li>
<li>\(\epsilon_i\) are independent \(N(0,\sigma_i^2)\)</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Variance-covariance matrix</h2></hgroup><article  id="variance-covariance-matrix">

<p>\[Var(\epsilon) = \begin{bmatrix}\sigma_1^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \sigma_n^2\end{bmatrix}\]</p>

<ul>
<li>OLS assumes equal variance: \(\sigma_1^2 =\dots = \sigma_n^2 = \sigma^2\)</li>
<li>Using OLS we would get unbiased estimation of the parameters</li>
<li>The OLS estimates no longer have minimum variance</li>
<li>We must account for unequal variance in the estimation process</li>
<li>Consider three cases:

<ul>
<li>Error variances are known (unrealistic)</li>
<li>Error variances are known up to proportionality constant</li>
<li>Error variances are known (realistic)</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Error variances are known</h2></hgroup><article  id="error-variances-are-known">

<p>Likelihood</p>

<p>\[L(\beta) = \prod\limits_{i=1}^n \frac{1}{\sqrt{2\pi\sigma_i^2}} \exp \left[ -\frac{1}{2\sigma_i^2} (Y_i -\beta_0 - \beta_1X_{i1} -\dots - \beta_{p-1}X_{i, p-1})^2\right]\]</p>

<p>Define \[w_i = \frac{1}{\sigma_i^2}\]</p>

<p>\[L(\beta) = \left[\prod\limits_{i=1}^n \frac{\sqrt{w_i}}{\sqrt{2\pi}}\right] \exp \left[ -\frac{1}{2} \sum\limits_{i=1}^n w_i(Y_i -\beta_0 - \beta_1X_{i1} -\dots - \beta_{p-1}X_{i, p-1})^2\right]\] Minimize</p>

<p>\[Q_w = \sum\limits_{i=1}^n w_i(Y_i -\beta_0 - \beta_1X_{i1} -\dots - \beta_{p-1}X_{i, p-1})^2\] ## Intuition</p>

<ul>
<li>\(w_i\) for \(i=1,\dots , n\) are the regression weights</li>
<li>In OLS \(w_i=1\) i.e. all observations get equal weights</li>
<li>Weight \(w_i\) are inversely proportional to the variance \(\sigma_i^2\)</li>
<li>Weights reflects the amount of information contained in the observations</li>
<li>An observation with higher variance gets smaller weight</li>
<li>More precise \(\Rightarrow\) More information \(\Rightarrow\) More weight</li>
</ul>

</article></slide><slide class=""><hgroup><h2>In matrix notation</h2></hgroup><article  id="in-matrix-notation">

<p>\[W = \begin{bmatrix}w_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; w_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; w_n\end{bmatrix}\]</p>

<p>Normal Equation</p>

<p>\[(X^TWX)\hat{\beta}_w = X^TWY\]</p>

<p>Estimators</p>

<p>\[\hat{\beta}_w = (X^TWX)^{-1}X^TWY\]</p>

<p>Variance of the estimators</p>

<p>\[Var(\hat{\beta}_w = (X^TWX)^{-1}\]</p>

</article></slide><slide class=""><hgroup><h2>Properties of the estimators</h2></hgroup><article  id="properties-of-the-estimators">

<ul>
<li>Unbiased</li>
<li>Consistent</li>
<li>Minimum variance among unbiased linear estimators</li>
<li>When weights are known Var(\(\hat{\beta}_w\)) is generally less than Var(\(\hat{\beta}\))</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Error variances unknown</h2></hgroup><article  id="error-variances-unknown">

<ul>
<li>We need to estimate the error variances.</li>
<li>Residuals from an OLS gives valuable information about the error variances</li>
<li>Two methods:

<ul>
<li>Estimation of variance function</li>
<li>Use of replicates or near replicates</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Estimation of variance</h2></hgroup><article  id="estimation-of-variance">

<ul>
<li>Squared residual \(\hat{\epsilon}^2\) is an estimator of \(\sigma_i^2\)</li>
<li>Absolute residual \(|\hat{\epsilon}|\) is an estimator for \(\sigma_i\)</li>
<li>Idea

<ul>
<li>We can estimate the variance function describing the relation of \(\sigma_i^2\) to relevant predictor variables by first fitting the regression model using unweighted least squares and then regressing \(\hat{\epsilon}^2\) or \(|\hat{\epsilon}|\) against the appropriate predictor variables.</li>
</ul></li>
<li>\(|\hat{\epsilon}|\) is preferred if outliers exist.</li>
</ul>

</article></slide><slide class=""><hgroup><h2>General guidelines</h2></hgroup><article  id="general-guidelines">

<ol>
<li><p>A residual plot against \(X_l\) exhibits a megaphone shape. \(\Rightarrow\) Regress the absolute residuals against \(X_l\)</p></li>
<li><p>A residual plot against \(\hat{Y}\) exhibits a megaphone shape. \(\Rightarrow\) Regress the absolute residuals against \(\hat{Y}\)</p></li>
<li><p>A plot of the squared residuals against \(X_l\) exhibits an upward tendency. \(\Rightarrow\) Regress the squared residuals against \(X_l\)</p></li>
<li><p>A plot of the squared residuals against \(X_l\) suggests that the variance increases rapidly with increases in \(X_l\) up to a point and then increases more slowly. \(\Rightarrow\) Regress the absolute residuals against \(X_l\) and \(X_l^2\).</p></li>
</ol>

</article></slide><slide class=""><hgroup><h2>What next?</h2></hgroup><article  id="what-next">

<p>After the variance function or the standard deviation function is estimated, the fitted values from this function are used to obtain the estimated weights:</p>

<p>\[w_i = \frac{1}{\hat{s}_i^2}\quad \text{ where } \hat{s}_i \text{ is fitted value from standard deviation function}\] \[w_i = \frac{1}{\hat{v}_i}\quad \text{ where } \hat{v}_i \text{ is fitted value from variance function}\]</p>

<p>The parameters are then estiamted as</p>

<p>\[\hat{\beta}_w = (X^TWX)^{-1}X^TWY\]</p>

</article></slide><slide class=""><hgroup><h2>Use of Replicates or Near Replicates</h2></hgroup><article  id="use-of-replicates-or-near-replicates">

<ul>
<li><p>In designed experiments \(\sigma_i^2\) is estimated suing replicate observations at each combination of levels of the predictor variables.</p></li>
<li><p>In observation studies, near replicates many be used.</p></li>
<li><p>For example, if the residual plot against \(X_l\) shows a megaphone appearance, cases with \(X_1\) values can be grouped together and the variance of the residuals in each group calculated.</p>

<ul>
<li>The reciprocal of these variances are the weights.</li>
</ul></li>
</ul>

</article></slide><slide class=""><hgroup><h2>Example (Strong Interaction ALR4 page 157)</h2></hgroup><article  id="example-strong-interaction-alr4-page-157">

<ul>
<li>Response : scattering cross-section (\(y\)), Predictor: square of total energy in the center of mass frame of reference (\(s\))</li>
<li>Designed experiment</li>
<li>A very large number of particles was counted at each setting of \(s\)</li>
<li>The variance of \(y\) is thus known almost exactly</li>
</ul>

</article></slide><slide class=""><hgroup><h2>Example (Strong Interaction ALR4 page 157)</h2></hgroup><article  id="example-strong-interaction-alr4-page-157-1">

<pre >## 
## Call:
## lm(formula = y ~ x, data = alr4::physics, weights = 1/SD^2)
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.3230 -0.8842  0.0000  1.3900  2.3353 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  148.473      8.079   18.38 7.91e-08 ***
## x            530.835     47.550   11.16 3.71e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.657 on 8 degrees of freedom
## Multiple R-squared:  0.9397, Adjusted R-squared:  0.9321 
## F-statistic: 124.6 on 1 and 8 DF,  p-value: 3.71e-06</pre>

</article></slide><slide class=""><hgroup><h2>Example (Blood pressure)</h2></hgroup><article  id="example-blood-pressure">

<p><img src="diagnostics_files/figure-html/unnamed-chunk-14-1.png" width="960" style="display: block; margin: auto;" /></p>

<ol>
<li>\(\Rightarrow\) linear relationship (unweighted)</li>
<li>\(\Rightarrow\) confirms the nonconstant error variance</li>
<li>\(\Rightarrow\) a linear relation between Age and standard error is reasonable</li>
</ol>

</article></slide><slide class=""><hgroup><h2>Example (Blood pressure)</h2></hgroup><article  id="example-blood-pressure-1">

<ul>
<li>Regress absolute residuals against <code>Age</code></li>
</ul>

<pre >##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept) -1.54948    2.18692 -0.7085 0.4817858
## blood$Age    0.19817    0.05309  3.7328 0.0004705
## 
## n = 54, p = 2, Residual SE = 4.46057, R-Squared = 0.21</pre>

<ul>
<li>Variance function</li>
</ul>

<p>\[\hat{s} = -1.5494776 + 0.1981723 Age\]</p>

<ul>
<li>Weights</li>
</ul>

<pre class = 'prettyprint lang-r'>w = 1/lmod_abs_res$fitted.values^2
head(w)</pre>

<pre >##          1          2          3          4          5          6 
## 0.06920928 0.14655708 0.12661657 0.09725115 0.08625993 0.11048521</pre>

<pre class = 'prettyprint lang-r'>head(blood$Age)</pre>

<pre >## [1] 27 21 22 24 25 23</pre>

</article></slide><slide class=""><hgroup><h2>Example (Blood pressure)</h2></hgroup><article  id="example-blood-pressure-2">

<ul>
<li>OLS</li>
</ul>

<pre >##              Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept) 56.156929   3.993674 14.0615 &lt; 2.2e-16
## Age          0.580031   0.096951  5.9827  2.05e-07
## 
## n = 54, p = 2, Residual SE = 8.14575, R-Squared = 0.41</pre>

<ul>
<li>WLS</li>
</ul>

<pre class = 'prettyprint lang-r'>wls_mod = lm(BP ~ Age, weights = w, data = blood)
sumary(wls_mod)</pre>

<pre >##              Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept) 55.565766   2.520918  22.042 &lt; 2.2e-16
## Age          0.596342   0.079238   7.526 7.187e-10
## 
## n = 54, p = 2, Residual SE = 1.21302, R-Squared = 0.52</pre></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
